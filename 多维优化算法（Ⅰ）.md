# Python实现多维优化算法（Ⅰ）

---

## 为什么使用Python？

处理多维函数时经常会遇到向量、矩阵乘法等，而这些运算的结果经常会超过C语言double变量类型的上限，因此尽管C语言可以轻松处理一维优化问题，对于多维问题便似乎“束手无策”了。
Python则可以轻松的处理大数，因此可以很方便的用于多维优化算法的实现。当然，本系列的所有程序都只应用Python最基本的语法，不涉及NumPy等库的应用。

---

## 最速下降法

最速下降法的核心是步长α的计算。一般来说α应该运用一维优化算法来求解，但由于：
1. 每一步关于α的函数都会改变，通过编程求导目前的能力还无法做到，而且计算开销估计会很大。
2. 即便使用斐波那契法等不需要求导的一阶优化算法，我们也需要预先给定最优解可能位于的区间，并且目标函数在给定的区间上还必须是单峰函数。而这样的区间很难在每一步迭代前预先划定。

因此我们不得不利用 $ α=\frac{g^{(k)^{T}}g^{(k)}}{g^{(k)^{T}}F(x^{(k)})g^{(k)}} $ 来近似求解α.

**实现代码如下：**
~~~Python
def Solve_Grid(x):
    """此函数负责求解梯度"""
    g=[float(4*(x[0]-4)*(x[0]-4)*(x[0]-4)),2*(x[1]-3),16*(x[2]+5)*(x[2]+5)*(x[2]+5)]
    return g

def Solve_H(x):
    """此函数负责求解黑塞矩阵"""
    H=[[12*(x[0]-4)*(x[0]-4),0.0,0],[0.0,2,0],[0.0,0,48*(x[2]+5)*(x[2]+5)]]
    return H

def Solve_α(g,H):
    """此函数负责求解每一步下降的步长α"""
    temp1=0.0
    temp2=[0.0,0,0]
    temp3=0.0

    for i in range(0,3):
        temp1=temp1+g[i]*g[i]

    for i in range(0,3):
        for j in range(0,3):
            temp2[i]=temp2[i]+g[j]*H[j][i]

    for i in range(0,3):
        temp3=temp3+temp2[i]*g[i]

    return temp1/temp3

if __name__=='__main__':

    x=[4.0,2,-1]#起始点
    n=0.1

    for i in range(0,10001):
        if i==10*n:
            print(f"第{i}次迭代后得到的结果为：{x}")
            n=n*10
        g=Solve_Grid(x)
        α=Solve_α(g,Solve_H(x))
        for j in range(0,3):
            x[j]=x[j]-α*g[j]
~~~

**实验结果分析：**

~~~
第1次迭代后得到的结果为：[4.0, 2.002604176574904, -2.333338406350827]
第10次迭代后得到的结果为：[4.0, 3.0037894604208017, -4.922372720282893]
第100次迭代后得到的结果为：[4.0, 3.000048558408501, -4.981890977677356]
第1000次迭代后得到的结果为：[4.0, 3.0000014372582333, -4.994397815412373]
第10000次迭代后得到的结果为：[4.0, 3.0000000451576905, -4.9982322239511054]
~~~

第10次迭代结果与第1次迭代结果有显著差异，而第10、100、1000与10000次迭代结果之间差异不大，说明最速下降法在前几次迭代中下降速度较快，而在接近最优解时下降速度变慢。
